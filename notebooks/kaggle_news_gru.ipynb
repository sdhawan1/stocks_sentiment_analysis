{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kaggle_news_gru.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcnL_3uhLZl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "  let's try and train a GRU here...\n",
        "\n",
        "  resources: https://blog.floydhub.com/gru-with-pytorch/ \n",
        "\n",
        "  TODO NEXT: modularize this code?\n",
        "'''\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYslTKI93X0O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "cebbcb5f-5db8-4b29-e33e-de367c276348"
      },
      "source": [
        "#first, fetch the dataset (headlines + stock movements)\n",
        "import pickle as pkl\n",
        "\n",
        "fpath = './drive/My Drive/Colab Notebooks/all_news_trdata/allstocks_trdata_8k.pkl'\n",
        "with open(fpath, 'rb') as f:\n",
        "  trdata = pkl.load(f)\n",
        "\n",
        "print(trdata.shape)\n",
        "print(type(trdata))\n",
        "print(trdata)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7489, 2)\n",
            "<class 'numpy.ndarray'>\n",
            "[['Benghazi Committee Releases Final Report, Slams Clinton. \\n In American Markets, Panic Begins to Subside - The New York Times. \\n Barack Obama: ‘Mr. Trump Embodies Global Elites’ Not Working Class - Breitbart. \\n Reactions to the Supreme Court Ruling on Texas’ Abortion Law - The New York Times. \\n Stacey Dash Talks ’Dash America’ Movement: Conservatives Can ’Take Back Influence and Power’ in Hollywood - Breitbart'\n",
            "  '1']\n",
            " ['‘Brexit’ Opens Uncertain Chapter in Britain’s Storied History - The New York Times. \\n Benghazi Committee Releases Final Report, Slams Clinton. \\n The 15 Questions About Benghazi Barack Obama Does Not Want To Answer - Breitbart. \\n Jenner on ’SI’ 40 Years Later: My ‘Macho Male’ Olympics Body Disguised ‘The Woman Living Inside Me’ - Breitbart. \\n Sharyl Attkisson: Obama and Clinton Lied to the Public, ‘Impeded the Investigation’ - Breitbart'\n",
            "  '1']\n",
            " ['Hamas Ends Week-Old Deal Importing Israeli Watermelons Into Gaza. \\n Demand for ‘Himalayan Viagra’ Fungus Heats Up, Maybe Too Much - The New York Times. \\n Ex-Colts RB Dies After Accidentally Shooting Himself - Breitbart. \\n Qatari Daily Publishes Ramadan Poem Outlining Jewish Plot. \\n C.I.A. Arms for Syrian Rebels Supplied Black Market, Officials Say - The New York Times'\n",
            "  '1']\n",
            " ...\n",
            " ['This calculator will tell you how many hours of activity you need to work off fast-food meals. \\n Donald Trump: I Would Not Order Military to Violate Laws, Treaties with Torture - Breitbart. \\n Priebus: ’The Odds of a Contested Convention Are Very Small’ - Breitbart. \\n Mayor-Turned-Child-Sex Offender Arrested at School--Again. \\n WATCH: Actor Keanu Reeves Shreds Targets Shooting 3-Gun - Breitbart'\n",
            "  '1']\n",
            " ['Levin Criticizes Candidates ’Beating Down Opponents’ With Vulgar ’Ruthlessness’ - Breitbart. \\n UPDATE: Hispanic Man to Be Buried in South Texas ’Whites Only\"’Cemetery. \\n College Frat Party Behind Standoff with Police at Texas Border Ranch. \\n Hillary Laughs Off FBI Investigation: It’s a ’Security Review’ - Breitbart. \\n Donald Trump Drops Out of Scheduled CPAC Appearance - Breitbart'\n",
            "  '1']\n",
            " ['Donald Trump and Ted Cruz Square Off in Super Saturday Caucuses - Breitbart. \\n CPAC Boos John Kasich as Michelle Malkin Slams His Support for Common Core, Attacks on Detractors - Breitbart. \\n We’ve entered an entirely new phase of the China debate. \\n Raheel Raza: I Need Defense Against Fellow Muslims Trying To Kill Me, We Can’t Be PC - Breitbart. \\n ’Path to 9/11’ Director Cyrus Nowrasteh to Premiere ’The Young Messiah’ at CPAC - Breitbart'\n",
            "  '1']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a8TeXv5RMDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "#run file with all dependencies.\n",
        "%run './drive/My Drive/Colab Notebooks/news_fns.py'\n",
        "\n",
        "titles = trdata[:,0]\n",
        "labels = trdata[:,1]\n",
        "train_loader, val_loader, titles_tok_padded, title_lens, hash_size = create_dataset_dataloader(titles, labels)\n",
        "\n",
        "#need hash_size when initializing gru..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N5mUCFdUDSW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "\n",
        "'''\n",
        "[this is all working fine]\n",
        "source: https://blog.floydhub.com/gru-with-pytorch/\n",
        "pytorch documentation (not very good): https://pytorch.org/docs/master/nn.html#gru\n",
        "'''\n",
        "\n",
        "\n",
        "def create_emb_layer(num_embeddings, embedding_dim, emb_wts = None, non_trainable=False):\n",
        "    if emb_wts is None:\n",
        "        embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
        "    else:\n",
        "        num_embeddings, embedding_dim = emb_wts.shape\n",
        "        embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
        "        embedding.load_state_dict(emb_wts)\n",
        "        if non_trainable:\n",
        "            embedding.weight.requires_grad = False\n",
        "    return embedding\n",
        "\n",
        "#create a GRU class\n",
        "class GRUNet(nn.Module):\n",
        "    \n",
        "    '''\n",
        "        input parameters:\n",
        "            input_dim (gru): size of the input embeddings (~50?)\n",
        "            hidden_dim (gru/affine): size of the hidden parameter (50) = output of the gru = input to the affine layer.\n",
        "            output_dim (affine): size of the output (2) = final num_classes\n",
        "            n_layers_gru: number of stacked layers of the GRU (1 or 2) ~ how to make bidirectional?\n",
        "            \n",
        "            embedding_dim (embed): should be the same as input_dim (size of the)\n",
        "            emb_wts (embed): matrix of pretrained embeddings (GloVe)\n",
        "            num_embeddings (embed): total vocab size...\n",
        "    '''\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers_gru, drop_prob=0.2,\n",
        "                num_embeddings = 0, embedding_dim = 0, emb_wts = None, non_trainable=False):\n",
        "        super(GRUNet, self).__init__() #initialize the super-class?\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers_gru\n",
        "        \n",
        "        #we need an embedding layer, a gru layer, and maybe a FC layer\n",
        "        #create an embedding layer here.\n",
        "        self.embedding = create_emb_layer(num_embeddings, embedding_dim, emb_wts, non_trainable)             \n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers_gru, batch_first=True, dropout=drop_prob) #needs batchfirst??\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    #todo: rewrite this to remove padding\n",
        "    def forward(self, x, lens, h):\n",
        "        out = self.embedding(x)\n",
        "        #remove padding\n",
        "        x_packed = pack_padded_sequence(out, lens, batch_first=True, enforce_sorted=False)\n",
        "        out, h = self.gru(x_packed, h)\n",
        "        #VERY IMPORTANT: hidden dimension (h) is the final output of every sequence!\n",
        "        out = self.relu(h)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "    \n",
        "    def init_hidden(self, batch_size, device='cpu'):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
        "        return hidden\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEHTnq9ekYab",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "b38ddc24-c7b9-46e3-a5d1-fbb8e3663645"
      },
      "source": [
        "# Create a training & validation procedure:\n",
        "\n",
        "'''\n",
        "  training:\n",
        "    - load a batch\n",
        "    - do a forward pass\n",
        "    - do a backward pass with the given \"criterion\" loss\n",
        "      ~ remember you have to detach gradients ***\n",
        "      ~ remember you have to zero_gradient\n",
        "    \n",
        "  validation:\n",
        "    - load & run all batches\n",
        "    - calculate the validation accuracy after every epoch.\n",
        "'''\n",
        "\n",
        "import time\n",
        "\n",
        "# ========================================\n",
        "#               Validation\n",
        "# ========================================\n",
        "\n",
        "#given logits and labels, return an accuracy\n",
        "def flat_accuracy(logits, labels):\n",
        "  labels = labels.numpy()\n",
        "  preds = logits.detach().numpy()\n",
        "  pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "  #print(\"\\npreds:\")\n",
        "  #print(pred_flat)\n",
        "  #print(labels)\n",
        "  #print(np.sum(pred_flat == labels))\n",
        "  return 1.0 * np.sum(pred_flat == labels) / len(labels)\n",
        "\n",
        "def validate(model, data_loader):\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  model.eval() #put model in evaluation mode...\n",
        "  eval_loss, eval_acc, ctr = 0,0,0\n",
        "  for batch in data_loader:\n",
        "    titles, lengths, labels = batch['title'], batch['length'], batch['label']\n",
        "    bsize = titles.shape[0]\n",
        "    h0 = model.init_hidden(bsize)\n",
        "    logits = model(titles, lengths, h0) #also add lengths to do pack_padded_seq\n",
        "\n",
        "    tl = torch.tensor([0 if l=='-1' else 1 for l in labels]) ##TODO: FIX IN PREPROCESSING. Convert to integers, either 0 or 1\n",
        "    loss = criterion(logits.view(bsize, 2), tl)\n",
        "    acc = flat_accuracy(logits.view(bsize, 2), tl)\n",
        "    eval_loss += loss\n",
        "    eval_acc += acc\n",
        "  #return final accuracy\n",
        "  return eval_loss/len(data_loader), eval_acc/len(data_loader)\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "# ========================================\n",
        "#               Training\n",
        "# ========================================\n",
        "\n",
        "#earlier variables needed: hash_size, learn_rate\n",
        "#utilize the adam optimizer...\n",
        "\n",
        "#initialize model\n",
        "input_dim, hidden_dim, num_classes = 50, 50, 2\n",
        "n_layers_gru = 1\n",
        "\n",
        "learn_rate = 0.001\n",
        "num_epochs = 2  #let's say between 2 & 4.\n",
        "\n",
        "gru = GRUNet(input_dim, hidden_dim, num_classes, n_layers_gru, 0.2, hash_size, 50)\n",
        "\n",
        "#loss (https://pytorch.org/docs/stable/nn.html#crossentropyloss)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(gru.parameters(), lr = learn_rate)\n",
        "\n",
        "\n",
        "gru.train()\n",
        "start_time = time.clock()  \n",
        "for epoch in range(num_epochs):\n",
        "  cum_loss = 0.0\n",
        "  counter = 0\n",
        "\n",
        "  for batch in train_loader:\n",
        "    titles, lengths, labels = batch['title'], batch['length'], batch['label']\n",
        "    bsize = titles.shape[0]\n",
        "\n",
        "    #zero grad (??)\n",
        "    gru.zero_grad()\n",
        "\n",
        "    #run a forward pass\n",
        "    h0 = gru.init_hidden(bsize)\n",
        "    logits = gru(titles, lengths, h0) #also add lengths to do pack_padded_seq\n",
        "\n",
        "    tl = torch.tensor([0 if l=='-1' else 1 for l in labels]) ##TODO: FIX IN PREPROCESSING. Convert to integers, either 0 or 1\n",
        "    loss = criterion(logits.view(bsize, 2), tl)\n",
        "\n",
        "    #back pass\n",
        "    loss.backward()\n",
        "\n",
        "    cum_loss += loss.item()\n",
        "    counter += 1\n",
        "    if (counter % 50) == 0:\n",
        "      print(\"Epoch {}........ Step {}/{}............ Avg Loss: {}\".format(epoch, counter, len(train_loader), cum_loss/counter))\n",
        "  \n",
        "  ### at the end of the epoch, compute validation accuracy\n",
        "  val_loss, val_acc = validate(gru, val_loader)\n",
        "  el_time = time.clock() - start_time\n",
        "  print(\"Epoch {}/{} done, validation accuracy: {}, elapsed time: {}\".format(epoch+1, num_epochs, val_acc, el_time))\n",
        "\n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0........ Step 50/211............ Avg Loss: 0.698989679813385\n",
            "Epoch 0........ Step 100/211............ Avg Loss: 0.6994273614883423\n",
            "Epoch 0........ Step 150/211............ Avg Loss: 0.7005022303263346\n",
            "Epoch 0........ Step 200/211............ Avg Loss: 0.7007146316766739\n",
            "Epoch 1/2 done, validation accuracy: 0.4282852564102564, elapsed time: 16.717280999999986\n",
            "Epoch 1........ Step 50/211............ Avg Loss: 0.698989679813385\n",
            "Epoch 1........ Step 100/211............ Avg Loss: 0.6994273614883423\n",
            "Epoch 1........ Step 150/211............ Avg Loss: 0.7005022303263346\n",
            "Epoch 1........ Step 200/211............ Avg Loss: 0.7007146316766739\n",
            "Epoch 2/2 done, validation accuracy: 0.4282852564102564, elapsed time: 33.413320999999996\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pss9Cz3EHY3P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "4c7fc0c0-856e-4621-cf5a-f7ea5d24e517"
      },
      "source": [
        "'''\n",
        "val_loss, val_acc = validate(gru, val_loader)\n",
        "el_time = time.clock() - start_time\n",
        "print(\"Epoch {}/{} done, validation accuracy: {}, elapsed time: {}\".format(epoch+1, num_epochs, val_acc, el_time))\n",
        "'''\n",
        "\n",
        "#given logits and labels, return an accuracy\n",
        "def flat_accuracy(logits, labels):\n",
        "  labels = labels.numpy()\n",
        "  preds = logits.detach().numpy()\n",
        "  pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "  #print(\"\\npreds:\")\n",
        "  #print(pred_flat)\n",
        "  #print(labels)\n",
        "  #print(np.sum(pred_flat == labels))\n",
        "  return 1.0 * np.sum(pred_flat == labels) / len(labels)\n",
        "\n",
        "def validate(model, data_loader):\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  model.eval() #put model in evaluation mode...\n",
        "  eval_loss, eval_acc, ctr = 0,0,0\n",
        "  for batch in data_loader:\n",
        "    titles, lengths, labels = batch['title'], batch['length'], batch['label']\n",
        "    bsize = titles.shape[0]\n",
        "    h0 = model.init_hidden(bsize)\n",
        "    logits = model(titles, lengths, h0) #also add lengths to do pack_padded_seq\n",
        "\n",
        "    tl = torch.tensor([0 if l=='-1' else 1 for l in labels]) ##TODO: FIX IN PREPROCESSING. Convert to integers, either 0 or 1\n",
        "    loss = criterion(logits.view(bsize, 2), tl)\n",
        "    acc = flat_accuracy(logits.view(bsize, 2), tl)\n",
        "    eval_loss += loss\n",
        "    eval_acc += acc\n",
        "  #return final accuracy\n",
        "  return eval_loss/len(data_loader), eval_acc/len(data_loader)\n",
        "\n",
        "print(type(gru))\n",
        "print(type(val_loader))\n",
        "loss, acc = validate(gru, val_loader)\n",
        "print(loss)\n",
        "print(acc)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class '__main__.GRUNet'>\n",
            "<class 'torch.utils.data.dataloader.DataLoader'>\n",
            "tensor(0.7323, grad_fn=<DivBackward0>)\n",
            "0.4476161858974359\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkGEoTHWPN1B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "33e6ba8d-cfe9-4d50-a509-b1d3d060c1eb"
      },
      "source": [
        "pred_flat"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-94-b585fdf25537>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred_flat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'pred_flat' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oVgxTjd9kEI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "9aae7897-d9d3-4ab6-da34-e7284164785f"
      },
      "source": [
        "print(type(labels))\n",
        "print(type(labels[0]))\n",
        "print(logits.size())\n",
        "\n",
        "l0 = [int(l) for l in labels]\n",
        "print(torch.tensor(l0))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "<class 'numpy.str_'>\n",
            "torch.Size([1, 32, 2])\n",
            "tensor([-1,  1,  1,  1,  1, -1,  1, -1, -1, -1, -1,  1, -1, -1,  1,  1, -1, -1,\n",
            "        -1,  1,  1,  1,  1,  1, -1,  1, -1, -1,  1, -1,  1, -1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gjtiaxMo5F7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d29a414e-76d7-4b30-a10e-f206f9262d34"
      },
      "source": [
        "# step by step, \"informal\" journey through the neural net.\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from keras.preprocessing.text import one_hot\n",
        "import numpy as np\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "\n",
        "\n",
        "### first, start by preprocessing & creating input\n",
        "y = [0, 1]\n",
        "x_sentlist = [\"a quick brown fox jumps\", \"a lazy brown dog barks loudly\"]\n",
        "allwds = '. '.join(x_sentlist)\n",
        "vocab_size = len(set( text_to_word_sequence(allwds) ))\n",
        "hash_size = vocab_size * 10\n",
        "\n",
        "#first, tokenize x (start w/ batchsize 2)\n",
        "batch_size=2\n",
        "input_sents = []\n",
        "for sent in x_sentlist:\n",
        "  input_sents.append(torch.tensor(one_hot(sent, hash_size)))\n",
        "print(input_sents)\n",
        "\n",
        "# add padding & convert to torch tensor\n",
        "lens = [len(s) for s in input_sents]\n",
        "sents_tok_padded = pad_sequence(input_sents, batch_first=True, padding_value=0)\n",
        "print(\"with padding included...\")\n",
        "print(sents_tok_padded)\n",
        "\n",
        "x = torch.tensor(sents_tok_padded)\n",
        "\n",
        "\n",
        "#---------------------------------------------#\n",
        "### next, put the input x into the GRU net.\n",
        "\n",
        "print(\"inputs...\")\n",
        "print(x)\n",
        "print(lens)\n",
        "print(hash_size)\n",
        "\n",
        "#initialize \"neural net\" layer\n",
        "emb_size, hidden_dim, drop_prob = 50, 50, 0.2\n",
        "embedding = nn.Embedding(hash_size, emb_size)\n",
        "gru_lyr = nn.GRU(emb_size, hidden_dim, 1, batch_first=True, dropout=drop_prob) #needs batchfirst??\n",
        "fc_lyr = nn.Linear(hidden_dim, 2)\n",
        "\n",
        "\n",
        "### let's start the forward prop...\n",
        "\n",
        "x_emb = embedding(x)\n",
        "#print(x_emb)\n",
        "print(x_emb.shape) #2 x 6 x 50\n",
        "\n",
        "#run this through a pack_padded_seq operator (this output is correct, it just doesn't look correct)\n",
        "p0 = pack_padded_sequence(x_emb, lens, batch_first=True, enforce_sorted=False)\n",
        "x_packed, b, _, _ = p0\n",
        "print(x_packed)\n",
        "#print(x_packed[0].shape)\n",
        "\n",
        "### very important note: hf = the final output for each sequence!!! (we need to pass this on!)\n",
        "gru_op, hf = gru_lyr(p0)\n",
        "\n",
        "#pass the final output through an affine layer\n",
        "output = fc_lyr(hf)\n",
        "\n",
        "print(output)\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inputs...\n",
            "tensor([[78, 13, 52, 40, 19,  0],\n",
            "        [78, 49, 52,  5, 81, 61]])\n",
            "[5, 6]\n",
            "35105\n",
            "torch.Size([2, 6, 50])\n",
            "tensor([[ 1.0150e+00, -1.1883e+00,  4.2255e-01,  8.1253e-01,  1.2233e-01,\n",
            "         -3.1855e-01, -8.8173e-01, -4.9370e-01,  1.4209e+00, -2.5932e-01,\n",
            "          1.4959e+00, -9.0945e-01, -1.8885e-01, -2.7176e+00, -9.1975e-01,\n",
            "          1.1334e+00, -3.9781e-01,  2.9156e-01, -1.0414e+00,  8.4001e-01,\n",
            "          7.5788e-02,  1.2013e+00,  4.7893e-01, -5.8157e-01,  7.9253e-01,\n",
            "          1.4535e+00, -3.4666e-01,  7.5556e-01,  6.6904e-02, -8.8769e-01,\n",
            "          8.2614e-01, -2.0586e+00,  8.0433e-01,  6.8744e-01, -1.0236e-01,\n",
            "          1.3137e+00,  3.2567e-01, -1.1686e+00,  2.5810e-01,  1.8774e-01,\n",
            "          1.2054e+00, -9.0501e-01, -1.4245e+00, -1.1421e+00, -1.1427e+00,\n",
            "          4.4023e-01, -9.0785e-01, -7.0045e-01, -1.0885e+00,  9.2201e-02],\n",
            "        [ 1.0150e+00, -1.1883e+00,  4.2255e-01,  8.1253e-01,  1.2233e-01,\n",
            "         -3.1855e-01, -8.8173e-01, -4.9370e-01,  1.4209e+00, -2.5932e-01,\n",
            "          1.4959e+00, -9.0945e-01, -1.8885e-01, -2.7176e+00, -9.1975e-01,\n",
            "          1.1334e+00, -3.9781e-01,  2.9156e-01, -1.0414e+00,  8.4001e-01,\n",
            "          7.5788e-02,  1.2013e+00,  4.7893e-01, -5.8157e-01,  7.9253e-01,\n",
            "          1.4535e+00, -3.4666e-01,  7.5556e-01,  6.6904e-02, -8.8769e-01,\n",
            "          8.2614e-01, -2.0586e+00,  8.0433e-01,  6.8744e-01, -1.0236e-01,\n",
            "          1.3137e+00,  3.2567e-01, -1.1686e+00,  2.5810e-01,  1.8774e-01,\n",
            "          1.2054e+00, -9.0501e-01, -1.4245e+00, -1.1421e+00, -1.1427e+00,\n",
            "          4.4023e-01, -9.0785e-01, -7.0045e-01, -1.0885e+00,  9.2201e-02],\n",
            "        [-1.0613e+00,  1.0896e+00, -1.0150e+00, -2.4753e+00, -2.1535e-01,\n",
            "          1.5163e-01, -7.2199e-01,  8.8798e-01, -1.2311e+00, -1.2820e+00,\n",
            "         -1.0831e+00, -2.3285e-01, -5.3513e-01, -1.5833e-01, -7.9812e-01,\n",
            "          1.1709e+00, -1.5352e-01,  1.5399e+00,  7.4627e-01, -2.6922e+00,\n",
            "          6.4947e-01,  7.8227e-01,  6.7623e-01,  1.4484e+00,  4.4482e-01,\n",
            "         -4.5285e-01, -1.0863e+00,  9.7013e-01,  3.5969e-01,  1.0253e+00,\n",
            "          1.9002e-01,  6.1763e-01, -6.4437e-01,  5.9212e-01,  5.0797e-01,\n",
            "          3.1395e-01,  2.0929e-01,  1.1006e+00, -4.7634e-01,  4.1317e-01,\n",
            "          6.1819e-01, -1.9652e+00,  3.2959e-01,  1.4768e-01, -1.1452e+00,\n",
            "         -1.1114e-01,  9.0355e-01, -8.9533e-01,  1.5810e+00,  7.7884e-01],\n",
            "        [-1.1036e+00,  6.9690e-01, -2.7624e-01, -5.3172e-01,  3.5247e-01,\n",
            "         -1.2639e+00,  9.9424e-01, -1.1023e+00,  1.2668e+00,  4.8974e-01,\n",
            "         -1.6939e+00,  5.9406e-01, -1.6072e-02, -1.0504e+00, -4.3936e-01,\n",
            "         -1.7146e-01,  1.8443e-01, -1.3847e+00,  3.6780e-02,  1.8331e-01,\n",
            "          1.9697e-01, -9.8357e-01, -9.1818e-01, -2.3623e-02, -1.5548e+00,\n",
            "          1.1035e+00,  9.1759e-01,  6.3151e-01,  2.9969e-01, -5.4582e-01,\n",
            "          7.5133e-01, -4.4314e-02,  7.9051e-01, -9.8537e-01, -4.0084e-01,\n",
            "         -1.1061e+00, -9.4279e-01,  4.5829e-01, -2.4087e-01,  4.2140e-01,\n",
            "         -2.6683e-01,  2.5939e-01, -7.5553e-01,  1.5314e+00,  6.0109e-01,\n",
            "         -8.2435e-01, -1.0975e+00,  2.3377e-01, -6.3051e-01,  5.0903e-01],\n",
            "        [-1.1042e-01, -1.1802e+00,  6.9740e-01, -2.4483e-01,  3.7492e-01,\n",
            "         -1.0137e-01, -2.3090e+00, -3.0284e-01, -5.3903e-01,  1.9296e-01,\n",
            "          1.1955e+00, -3.7647e-01,  8.4693e-01,  3.3165e-01,  1.7923e-01,\n",
            "          3.0886e-01, -7.3113e-02,  9.0861e-01, -9.9996e-01,  9.9881e-01,\n",
            "         -9.4051e-01,  7.1192e-01, -2.0158e+00, -4.0000e-01,  1.8870e-01,\n",
            "         -1.4384e+00, -1.6326e+00,  9.8986e-02,  1.3293e+00,  5.3546e-01,\n",
            "          1.6493e-01,  3.3194e-01,  1.5184e+00, -1.1633e+00,  1.6109e-01,\n",
            "          2.2925e+00,  2.0706e-01, -2.3014e-01, -3.0418e-02, -1.2023e+00,\n",
            "         -5.3190e-01,  8.8345e-01,  1.3370e-01, -4.0406e-01,  1.3067e-01,\n",
            "          2.1748e+00,  4.5838e-01, -5.9821e-01, -5.4457e-01,  1.7928e+00],\n",
            "        [-1.1042e-01, -1.1802e+00,  6.9740e-01, -2.4483e-01,  3.7492e-01,\n",
            "         -1.0137e-01, -2.3090e+00, -3.0284e-01, -5.3903e-01,  1.9296e-01,\n",
            "          1.1955e+00, -3.7647e-01,  8.4693e-01,  3.3165e-01,  1.7923e-01,\n",
            "          3.0886e-01, -7.3113e-02,  9.0861e-01, -9.9996e-01,  9.9881e-01,\n",
            "         -9.4051e-01,  7.1192e-01, -2.0158e+00, -4.0000e-01,  1.8870e-01,\n",
            "         -1.4384e+00, -1.6326e+00,  9.8986e-02,  1.3293e+00,  5.3546e-01,\n",
            "          1.6493e-01,  3.3194e-01,  1.5184e+00, -1.1633e+00,  1.6109e-01,\n",
            "          2.2925e+00,  2.0706e-01, -2.3014e-01, -3.0418e-02, -1.2023e+00,\n",
            "         -5.3190e-01,  8.8345e-01,  1.3370e-01, -4.0406e-01,  1.3067e-01,\n",
            "          2.1748e+00,  4.5838e-01, -5.9821e-01, -5.4457e-01,  1.7928e+00],\n",
            "        [ 9.6579e-02, -3.9592e-01,  3.4966e-01, -9.2432e-01, -5.6022e-01,\n",
            "         -8.3551e-01,  1.5183e+00, -9.6008e-01, -5.4396e-01,  1.2086e+00,\n",
            "         -1.1621e+00, -4.7755e-01,  1.5340e-02,  1.4508e+00, -7.8008e-01,\n",
            "         -1.2367e+00, -6.0170e-01, -2.3377e+00, -7.3848e-01,  6.1513e-01,\n",
            "          1.1037e+00,  1.0173e+00,  6.9998e-01,  1.0941e+00,  1.8700e+00,\n",
            "          1.2900e+00, -1.0164e+00,  1.4663e-01,  1.0241e+00, -2.6802e-01,\n",
            "          4.4353e-01, -2.6221e+00,  4.5671e-01,  8.6558e-01,  1.3270e+00,\n",
            "         -1.4489e+00, -7.6593e-02,  6.9775e-01,  8.6175e-01, -4.9699e-04,\n",
            "          2.1799e-01,  1.7274e-01,  6.2277e-01, -4.6855e-01, -6.2193e-01,\n",
            "         -1.1081e+00,  3.0541e-01,  6.5858e-01, -5.0159e-01,  4.0610e-02],\n",
            "        [ 3.9993e-01, -3.2355e-01,  1.5973e-01,  1.0681e-02,  1.7974e+00,\n",
            "          1.1875e+00, -6.2043e-01,  4.6948e-01, -1.2578e+00,  9.0717e-01,\n",
            "          1.0312e+00,  2.7346e-01, -2.2422e+00,  2.6051e+00,  1.1277e-01,\n",
            "          4.5964e-01, -1.2276e+00,  1.7637e-01, -1.3529e+00,  1.1177e+00,\n",
            "          1.1011e+00, -2.5213e+00, -9.5430e-01, -1.7235e+00,  1.1285e+00,\n",
            "         -1.3250e+00, -4.5993e-01, -8.2930e-01,  2.6287e-01,  5.7915e-01,\n",
            "         -5.0902e-01, -5.6424e-01,  7.8676e-01,  4.7415e-01, -5.0800e-01,\n",
            "          1.3596e+00, -1.1259e+00, -6.7127e-01, -2.6697e+00, -3.8173e-01,\n",
            "          1.6908e+00,  7.3784e-01, -4.0220e-01, -1.6939e+00, -1.9040e-02,\n",
            "         -1.2043e+00,  1.0795e+00,  1.8167e+00, -9.3129e-01,  1.4431e+00],\n",
            "        [-1.1148e+00, -4.3371e-01, -6.3592e-01,  9.9772e-01, -1.8580e-01,\n",
            "         -3.3477e-01,  1.1255e+00,  1.2445e+00, -5.0713e-01, -1.3038e+00,\n",
            "         -7.9455e-01, -1.0425e+00, -2.4693e+00, -2.3472e-01,  6.0985e-01,\n",
            "          7.8315e-01, -4.0579e-01, -9.5128e-01, -4.5772e-01,  7.8259e-01,\n",
            "          2.2569e-01, -3.0525e-01,  5.1880e-01,  8.8866e-01,  1.0609e+00,\n",
            "          1.4689e+00, -4.9219e-01, -7.0322e-01,  8.6738e-02,  8.2381e-02,\n",
            "         -1.5008e+00,  1.3334e+00,  1.1804e+00,  1.0177e+00,  9.6398e-01,\n",
            "         -1.0231e+00,  3.8680e-01,  2.4303e-01, -3.5246e-01, -1.0407e+00,\n",
            "         -1.6267e+00, -1.1040e+00, -3.7072e-01,  1.0772e+00, -1.1146e+00,\n",
            "         -6.7586e-01, -9.9372e-02,  6.8731e-01, -1.2966e+00,  4.6531e-01],\n",
            "        [ 2.4594e-01,  9.0487e-01,  6.4267e-01, -9.4216e-01, -2.5059e+00,\n",
            "          5.9101e-01,  4.6172e-01,  1.0012e-01,  6.1500e-01,  7.1845e-02,\n",
            "          1.2192e+00, -1.3053e+00,  9.4633e-01,  1.2132e+00, -8.8172e-01,\n",
            "          8.7437e-01,  4.1591e-01,  1.2826e+00, -1.0585e+00, -2.9097e-02,\n",
            "          9.1932e-01, -3.5257e-01, -1.7666e-02, -8.2046e-01,  1.7872e+00,\n",
            "         -1.3007e-01,  2.1524e-01, -7.0486e-01, -2.4003e-01,  7.4198e-01,\n",
            "         -1.9637e+00, -8.6771e-01, -9.9430e-01,  6.7669e-01,  7.2563e-01,\n",
            "         -8.0467e-01,  3.4497e-01,  2.1752e-01,  1.3803e+00, -1.0354e+00,\n",
            "         -3.5599e-01, -1.2177e+00,  1.3648e+00, -5.7279e-01, -4.9983e-01,\n",
            "          2.5934e-02,  8.5179e-01, -8.4737e-01, -1.5086e+00, -7.1894e-01],\n",
            "        [ 2.1681e-01, -4.7091e-01,  9.9287e-01,  6.0623e-01, -4.1166e-01,\n",
            "         -1.9660e+00,  3.4248e-01, -4.9245e-01, -9.9577e-01, -3.6443e-01,\n",
            "          6.3407e-01, -1.6683e+00, -3.1958e-01, -1.6602e+00, -8.2765e-01,\n",
            "         -2.1512e-01,  1.2904e+00, -9.7819e-01, -9.4128e-01, -4.5908e-01,\n",
            "          1.8373e+00, -3.1331e-01,  1.4087e+00, -7.7798e-01, -1.6995e-01,\n",
            "         -1.2006e+00, -2.1864e-02,  3.2019e+00, -4.4540e-01,  4.6477e-01,\n",
            "          2.6915e-01, -3.1919e-01,  7.9488e-01, -6.8552e-01, -3.5673e-01,\n",
            "          7.4235e-02,  9.8290e-01, -9.6738e-01,  1.3719e+00,  3.5435e-01,\n",
            "         -1.2981e+00, -1.1066e+00, -1.4687e+00,  9.5193e-01, -1.9207e-01,\n",
            "          1.6845e-01, -1.5654e-01,  2.1557e+00,  1.1036e+00,  9.6173e-01]],\n",
            "       grad_fn=<PackPaddedSequenceBackward>)\n",
            "PackedSequence(data=tensor([[ 2.2263e-01, -1.4198e-01,  2.1142e-01, -1.5466e-01,  1.1278e-01,\n",
            "          6.2731e-02, -1.1532e-01,  1.1313e-01,  5.7194e-02,  1.7560e-01,\n",
            "         -1.2203e-01, -1.5028e-01,  1.5813e-01,  7.3027e-02,  3.3820e-01,\n",
            "         -5.3974e-01, -2.0480e-01,  9.8045e-02, -8.0018e-02,  4.9763e-01,\n",
            "          1.9512e-01,  4.4167e-01,  1.9357e-01,  4.0788e-01,  1.3607e-01,\n",
            "         -1.5657e-02, -3.6076e-01, -4.5898e-01, -2.4308e-01, -2.2429e-01,\n",
            "          4.8251e-02,  1.7893e-01, -2.3814e-02,  3.1920e-01,  2.2511e-01,\n",
            "         -1.4531e-02,  3.9896e-01, -4.5403e-01,  2.0373e-01, -4.5454e-01,\n",
            "          1.9795e-01, -6.5457e-02,  1.8150e-01,  8.7711e-02,  1.9542e-01,\n",
            "         -1.9997e-01,  2.9063e-01,  1.6027e-02, -2.2986e-03, -3.6954e-01],\n",
            "        [ 2.2263e-01, -1.4198e-01,  2.1142e-01, -1.5466e-01,  1.1278e-01,\n",
            "          6.2731e-02, -1.1532e-01,  1.1313e-01,  5.7194e-02,  1.7560e-01,\n",
            "         -1.2203e-01, -1.5028e-01,  1.5813e-01,  7.3027e-02,  3.3820e-01,\n",
            "         -5.3974e-01, -2.0480e-01,  9.8045e-02, -8.0018e-02,  4.9763e-01,\n",
            "          1.9512e-01,  4.4167e-01,  1.9357e-01,  4.0788e-01,  1.3607e-01,\n",
            "         -1.5657e-02, -3.6076e-01, -4.5898e-01, -2.4308e-01, -2.2429e-01,\n",
            "          4.8251e-02,  1.7893e-01, -2.3814e-02,  3.1920e-01,  2.2511e-01,\n",
            "         -1.4531e-02,  3.9896e-01, -4.5403e-01,  2.0373e-01, -4.5454e-01,\n",
            "          1.9795e-01, -6.5457e-02,  1.8150e-01,  8.7711e-02,  1.9542e-01,\n",
            "         -1.9997e-01,  2.9063e-01,  1.6027e-02, -2.2986e-03, -3.6954e-01],\n",
            "        [-3.9332e-01, -1.1318e-01, -3.0855e-02, -3.1255e-01, -7.5329e-03,\n",
            "          6.4489e-02, -2.4077e-01, -3.0713e-02,  2.9951e-02, -1.3599e-01,\n",
            "         -2.9117e-01, -7.1730e-02, -1.7320e-01, -2.4042e-01,  3.9577e-02,\n",
            "         -2.0373e-01, -4.6924e-02,  4.6518e-01,  3.9725e-02,  2.4412e-01,\n",
            "          2.8993e-01, -2.8143e-01, -2.2428e-01,  4.2577e-01,  1.5288e-01,\n",
            "          3.2355e-02, -2.9580e-02, -1.7079e-01, -1.4458e-01, -5.7907e-01,\n",
            "         -1.3912e-01,  1.8904e-01,  2.5937e-01,  2.8165e-01, -7.3409e-02,\n",
            "         -3.2413e-01, -1.0541e-02, -3.2468e-01,  6.7039e-02, -7.3366e-01,\n",
            "          2.3618e-02,  1.6397e-01, -1.5247e-01, -4.8344e-01, -1.9610e-01,\n",
            "         -2.1593e-02,  4.9332e-01,  1.1700e-01, -3.7696e-01, -1.5539e-01],\n",
            "        [ 4.3332e-01,  9.4413e-02,  3.4375e-01,  1.7167e-01,  1.7848e-01,\n",
            "         -1.9826e-01, -2.9828e-01,  1.9986e-01,  2.9818e-01,  1.6266e-01,\n",
            "         -1.5085e-02,  3.4616e-01,  1.2240e-01,  1.7679e-01,  1.7241e-01,\n",
            "         -9.2654e-02, -3.6377e-01, -8.2807e-02, -2.8619e-01,  2.3879e-01,\n",
            "         -2.1536e-01,  3.4183e-01,  3.1863e-01,  4.0679e-02, -1.6756e-01,\n",
            "          3.4055e-02, -3.1497e-01, -1.2302e-01, -2.1176e-01, -2.8918e-01,\n",
            "          2.0460e-01,  3.7885e-01, -6.8593e-02,  1.7362e-01,  1.8235e-01,\n",
            "         -9.4791e-02, -3.9310e-02, -2.3032e-01,  4.1291e-02,  2.0859e-01,\n",
            "          8.5666e-02, -3.2877e-01,  4.8666e-01, -1.6148e-01, -2.2968e-02,\n",
            "          3.4844e-01, -5.3347e-01,  1.0281e-01,  2.5814e-01, -8.7476e-02],\n",
            "        [-8.6873e-02, -4.0333e-01,  2.2891e-01, -2.4530e-03, -1.1369e-01,\n",
            "         -2.4879e-01, -2.5173e-01, -1.4371e-01,  6.1457e-03, -9.3256e-02,\n",
            "         -2.8172e-01, -1.8029e-01, -1.7189e-01, -1.2710e-02,  2.6492e-01,\n",
            "         -3.0448e-01, -2.5242e-01,  3.7429e-01,  8.5101e-02,  2.4392e-01,\n",
            "          6.7410e-01,  9.9039e-02,  8.9386e-02,  3.7946e-01,  2.8069e-01,\n",
            "          1.1392e-02,  9.8581e-02, -3.5306e-02,  1.8860e-01, -3.9026e-01,\n",
            "         -6.9475e-02,  3.1258e-01,  5.4210e-02, -1.2894e-01, -8.6203e-02,\n",
            "          2.9128e-01,  7.3194e-02, -2.6007e-01,  1.8148e-01, -6.0001e-01,\n",
            "          1.4954e-01, -3.0527e-03,  1.9514e-01,  1.0046e-01,  1.1059e-01,\n",
            "         -3.3869e-01,  4.2312e-01, -1.5933e-01,  2.2938e-02, -2.1678e-01],\n",
            "        [ 3.8992e-01, -3.6239e-01,  2.9663e-01,  2.6333e-01, -2.4705e-02,\n",
            "         -3.7850e-01, -3.1721e-01,  6.7524e-02,  1.8032e-01,  1.0868e-01,\n",
            "         -1.2405e-01,  2.6756e-02, -2.9173e-02,  1.9545e-01,  3.7745e-01,\n",
            "         -3.0371e-01, -3.2263e-01, -9.8463e-02, -1.4632e-01,  2.2446e-01,\n",
            "          4.5275e-01,  3.1780e-01,  2.9236e-01,  1.3449e-01,  5.9813e-02,\n",
            "          1.3748e-01, -3.5346e-02,  4.9892e-03,  1.9723e-01, -2.3634e-01,\n",
            "          3.0001e-02,  4.0087e-01, -1.6396e-02, -2.9315e-01,  8.7628e-02,\n",
            "          3.0632e-01,  5.0700e-02, -1.9784e-01,  1.9847e-01,  7.6137e-02,\n",
            "          8.2446e-02, -3.1397e-01,  5.7848e-01,  2.5526e-01,  1.9906e-01,\n",
            "         -1.3410e-01, -2.8037e-01, -1.5497e-01,  2.4416e-01, -2.0989e-01],\n",
            "        [-2.7911e-01, -9.6797e-03,  6.7722e-01, -5.2330e-02,  1.1124e-02,\n",
            "         -2.9684e-01, -6.1011e-02,  3.4754e-02, -4.2260e-02,  2.3871e-01,\n",
            "         -3.0962e-01,  1.9137e-01, -8.2144e-02, -4.6946e-02, -4.7326e-02,\n",
            "          2.2210e-01, -4.3979e-01,  2.0414e-01, -1.1930e-01, -9.0668e-03,\n",
            "          5.5922e-01,  2.7266e-02, -2.9962e-01,  6.0343e-01, -1.6481e-01,\n",
            "          1.8035e-01, -3.1416e-01,  1.7861e-01,  1.9396e-01, -4.1484e-01,\n",
            "          8.1429e-02,  5.3003e-01,  2.2064e-01,  1.1426e-01,  3.2072e-01,\n",
            "         -3.5293e-01, -1.1268e-02, -6.7490e-03,  2.8332e-01, -4.4100e-01,\n",
            "          5.9546e-02, -1.5007e-01, -4.8123e-04,  2.6584e-01,  1.8301e-01,\n",
            "          1.6953e-01, -2.8089e-02, -4.3432e-02,  7.5245e-03, -1.9791e-01],\n",
            "        [ 2.1060e-02, -5.3085e-01,  3.9094e-01,  9.6847e-02, -1.1281e-01,\n",
            "         -1.9904e-01,  1.4410e-01, -1.0918e-02,  3.1482e-01, -6.7867e-02,\n",
            "         -4.3061e-01,  1.1312e-01,  4.6142e-01,  1.7578e-02,  1.7914e-01,\n",
            "         -3.1345e-01, -2.7661e-01, -2.7457e-01, -1.7632e-01, -4.5451e-01,\n",
            "          4.9209e-01,  1.7211e-02, -2.1822e-01,  1.9128e-01, -3.5111e-01,\n",
            "         -1.4821e-01,  2.0551e-01, -7.7714e-01,  3.1469e-01, -3.4470e-01,\n",
            "         -3.3995e-01,  5.4768e-01,  2.3585e-01, -3.2936e-02,  4.4713e-01,\n",
            "          2.7731e-01,  2.8720e-01, -8.6917e-02, -1.7458e-01, -2.0473e-01,\n",
            "         -1.3448e-01, -1.2788e-01,  6.6434e-01, -3.3751e-02,  4.4950e-01,\n",
            "         -1.8653e-01, -3.6389e-01,  7.1370e-03, -1.2939e-01, -3.6910e-01],\n",
            "        [-2.3889e-01, -1.1600e-01,  4.7147e-01, -1.2708e-01,  9.7525e-02,\n",
            "         -1.6860e-01,  1.6008e-01,  6.1905e-02, -2.3819e-01,  2.1491e-01,\n",
            "         -2.6157e-01,  1.5988e-01,  5.8142e-02, -1.0327e-02,  1.1906e-02,\n",
            "          1.1666e-01, -4.2620e-01, -2.0623e-01, -7.0222e-02, -2.7976e-01,\n",
            "          3.8376e-01, -3.2759e-01, -4.4295e-01,  5.2500e-01,  7.1120e-03,\n",
            "         -5.7308e-02, -3.0756e-01, -8.1084e-03,  1.5680e-01, -3.2743e-01,\n",
            "         -3.3719e-02, -1.2851e-01,  3.7362e-01,  2.0917e-01, -6.0376e-03,\n",
            "         -1.0517e-01,  5.2477e-03,  3.6510e-01,  1.3699e-01,  8.9693e-02,\n",
            "         -2.0764e-01, -3.2663e-01,  2.3612e-01, -1.9323e-01, -1.6125e-01,\n",
            "          3.4360e-02,  3.7442e-01,  2.7270e-02,  8.8102e-02,  1.9432e-01],\n",
            "        [-3.7900e-01, -5.7240e-01,  5.6093e-01, -5.9076e-02, -2.5189e-01,\n",
            "         -9.2741e-02,  8.8911e-02,  1.0450e-01,  2.2932e-01,  9.1096e-02,\n",
            "         -6.5082e-02,  1.8785e-01,  5.0594e-01,  5.7623e-02,  2.8563e-01,\n",
            "         -2.6757e-01, -3.0631e-01, -4.3695e-02, -2.5224e-01, -3.4538e-01,\n",
            "          4.0120e-01, -7.4261e-02,  7.8410e-02,  4.6676e-01,  6.0231e-02,\n",
            "          4.3675e-03, -2.5128e-01, -7.5728e-01,  2.3754e-01, -1.7927e-01,\n",
            "         -2.0523e-01,  3.5858e-01,  3.3335e-01,  3.0368e-01,  2.0630e-02,\n",
            "          1.1512e-01,  2.7660e-02,  1.1645e-01, -3.0042e-01, -4.5832e-01,\n",
            "         -2.3247e-01,  6.1183e-02,  5.4373e-02,  1.5706e-01,  2.7598e-01,\n",
            "         -4.1446e-01,  3.2702e-01,  1.5848e-01, -1.9128e-01,  6.4722e-02],\n",
            "        [-9.7739e-02, -5.8057e-02, -1.9357e-01, -2.9805e-01,  1.4180e-01,\n",
            "         -4.7262e-01,  3.2780e-01,  8.6891e-02,  3.7056e-02,  3.3405e-01,\n",
            "          1.9893e-01, -5.2503e-02,  2.9339e-01, -4.2388e-02,  2.5911e-01,\n",
            "         -1.1266e-01, -3.1785e-02, -3.0052e-02, -6.1135e-02,  4.4369e-02,\n",
            "          6.0888e-01, -3.2778e-01, -1.1351e-01,  4.1779e-01, -1.5085e-01,\n",
            "         -4.0973e-01, -3.0893e-01,  5.0418e-01, -3.7340e-01, -2.3534e-01,\n",
            "          1.7764e-02, -5.2516e-02,  1.0477e-01,  4.1470e-02,  1.9950e-01,\n",
            "         -2.2632e-01, -3.3788e-01, -2.4885e-01,  1.1162e-01,  2.9273e-01,\n",
            "          1.6507e-01, -8.9727e-02,  4.9604e-01, -3.7689e-02, -3.1166e-01,\n",
            "          2.3929e-01,  3.0976e-01, -1.2690e-01, -9.1774e-02,  5.7819e-04]],\n",
            "       grad_fn=<CatBackward>), batch_sizes=tensor([2, 2, 2, 2, 2, 1]), sorted_indices=tensor([1, 0]), unsorted_indices=tensor([1, 0]))\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqqc9Org9Ww5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "241a0843-b810-475c-9303-51d2cbe293d0"
      },
      "source": [
        "#next, pass the output into a fc layer\n",
        "output = fc_lyr(hf)\n",
        "print(output)\n",
        "\n",
        "# eureka: hf is the final output (which is what we needed)\n",
        "'''\n",
        "print(hf.shape)\n",
        "print(hf)\n",
        "'''\n",
        "#unpack the packed sequence\n",
        "'''from torch.nn.utils.rnn import pad_packed_sequence\n",
        "x_padded, sent_lens = pad_packed_sequence(x_packed, batch_first=True, padding_value=0.0)\n",
        "print(x_padded.shape)\n",
        "print(sent_lens)\n",
        "print(x_padded[0][0])\n",
        "'''"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.1547,  0.1246],\n",
            "         [-0.0638,  0.1120]]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from torch.nn.utils.rnn import pad_packed_sequence\\nx_padded, sent_lens = pad_packed_sequence(x_packed, batch_first=True, padding_value=0.0)\\nprint(x_padded.shape)\\nprint(sent_lens)\\nprint(x_padded[0][0])\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvV6AeeZyyI3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}