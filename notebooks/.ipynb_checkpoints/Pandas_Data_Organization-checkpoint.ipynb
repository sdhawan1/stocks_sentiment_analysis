{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['297_rss_20160418', '268_rss_20160418', '174_rss_20160408', '121_rss_20160408', '290_rss_20160418', '173_rss_20160408', '126_rss_20160408', '369_rss_20160419', '166_rss_20160408', '133_rss_20160408', '117_rss_20160407', '285_rss_20160418', '161_rss_20160408', '134_rss_20160408', '282_rss_20160418', '228_rss_20160418', '329_rss_20160419', '383_rss_20160419', '384_rss_20160419', '80_rss_20160404']\n",
      "                                              title description         date\n",
      "0   Williams Cos. shares fall on multiple headlines              18 Apr 2016\n",
      "1    Trump most unpopular major candidate ever—poll              18 Apr 2016\n",
      "2               Netflix plunges after weak guidance              18 Apr 2016\n",
      "3    IBM earnings beat, but revenue keeps declining              18 Apr 2016\n",
      "4  Hamm: Forget Doha, oil oversupply gone this year              18 Apr 2016\n",
      "(10660, 3)\n"
     ]
    }
   ],
   "source": [
    "'''goal of this notebook: use pandas to organize the data into training data.'''\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from datetime import datetime as dt\n",
    "\n",
    "#set directory and extract file contents in a loop\n",
    "datadir = './../test_rss'\n",
    "datafiles = os.listdir(datadir)\n",
    "print(datafiles[:20])\n",
    "max_tr_limit = 1000\n",
    "\n",
    "### open a file\n",
    "#f = open(datadir + '/' + '733_rss_20160519')\n",
    "#print(f.read())\n",
    "\n",
    "list_dfs = []\n",
    "for i, fname in enumerate(datafiles):\n",
    "    if i >= max_tr_limit:\n",
    "        break\n",
    "    \n",
    "    #open an rss file, convert its html elements to beautiful soup, and extract all links and metadata.\n",
    "    full_fpath = datadir + '/' + fname\n",
    "    with open(full_fpath,'rb') as f:\n",
    "        soup = BeautifulSoup(f.read())\n",
    "        #print(soup)\n",
    "\n",
    "        #for each element, need to store it in pandas form. {Can I do this any faster???}\n",
    "        items = soup.findAll(\"item\")\n",
    "        for item in items:\n",
    "            #note: was not able to get the link. That will require some extra work.\n",
    "            df_item = pd.DataFrame({\n",
    "                'title':[item.title.text.strip()],\n",
    "                'description':[item.description.text.strip()],\n",
    "                'date':[item.pubdate.text.strip()[5:16]]\n",
    "            })\n",
    "\n",
    "            #aggregate all \"item\"-dataframes.\n",
    "            list_dfs.append(df_item)\n",
    "\n",
    "rawitems_df = pd.concat(list_dfs, axis=0).reset_index(drop=True)\n",
    "print(rawitems_df.head())\n",
    "print(rawitems_df.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    \"sql\" steps:\n",
    "    1. for each ticker, pull out all articles related to that ticker.\n",
    "        i. ~ create new columns in our dataframe to say which tickers belong to which titles.\n",
    "        ii. also, need to figure out which points have enough data.\n",
    "    \n",
    "    2. reorganize the dataframe:\n",
    "        i. group by ticker and date. When you do this, every group will be a new data point.\n",
    "        ii. Combine this with the stock movement of the given stock on the given day.\n",
    "        iii. Now we should be ready for data analysis.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              title description         date  \\\n",
      "0   Williams Cos. shares fall on multiple headlines              18 Apr 2016   \n",
      "1    Trump most unpopular major candidate ever—poll              18 Apr 2016   \n",
      "2               Netflix plunges after weak guidance              18 Apr 2016   \n",
      "3    IBM earnings beat, but revenue keeps declining              18 Apr 2016   \n",
      "4  Hamm: Forget Doha, oil oversupply gone this year              18 Apr 2016   \n",
      "\n",
      "   facebook  netflix  apple  google  amazon  aramco  morgan_stanley  chase  \n",
      "0     False    False  False   False   False   False           False  False  \n",
      "1     False    False  False   False   False   False           False  False  \n",
      "2     False     True  False   False   False   False           False  False  \n",
      "3     False    False  False   False   False   False           False  False  \n",
      "4     False    False  False   False   False   False           False  False  \n",
      "\n",
      "\n",
      "res2:\n",
      "          date                                        list_titles  length  \\\n",
      "0  06 Apr 2016  [Zuck tries Facebook Live ... doesn't go well,...     4.0   \n",
      "1  07 Apr 2016  [The firm helping Facebook get $$$ in lost rev...     3.0   \n",
      "2  08 Apr 2016  [Facebook taking shady retailers 'very serious...    28.0   \n",
      "3  19 Apr 2016  [How one trader plans to make millions on Face...    18.0   \n",
      "4  08 Apr 2016  [Actually, Netflix did warn about this price h...     9.0   \n",
      "\n",
      "     ticker  \n",
      "0  facebook  \n",
      "1  facebook  \n",
      "2  facebook  \n",
      "3  facebook  \n",
      "4   netflix  \n"
     ]
    }
   ],
   "source": [
    "### Q: what is the best way to search within pandas?? \n",
    "##  https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.contains.html\n",
    "'''\n",
    "    This cell: convert the html data from raw data to data that is ready for ML:\n",
    "    Namely, in our input data for ML, we want one row for each ticker and each day.\n",
    "'''\n",
    "\n",
    "'''\n",
    "    Search for tickers within titles, and create a new column for each ticker.\n",
    "'''\n",
    "def ticker_label_columns(tickers, rawitems_df):\n",
    "    l1 = []\n",
    "    contains_ticker_cols = [rawitems_df]\n",
    "    for t1 in tickers:\n",
    "        contains_ti = l1.copy()\n",
    "        ticker_title = t1.replace(' ','_') #come up with another function, or a dictionary?\n",
    "        contains_ti = rawitems_df['title'].str.contains(t1, case=False, regex=False).rename(ticker_title)\n",
    "        contains_ticker_cols.append(contains_ti)\n",
    "    res1 = pd.concat(contains_ticker_cols, axis=1)\n",
    "    return res1\n",
    "\n",
    "\n",
    "'''\n",
    "    Group by ticker and date. Combine all the titles into a single list of strings type.\n",
    "    ##! need to test this out on a bigger dataset\n",
    "'''\n",
    "def concat_titles_tk_date(res1, tickers):\n",
    "    ticker_frames = []\n",
    "    for t1 in tickers:\n",
    "        ticker_title = t1.replace(' ','_') ## need something better!\n",
    "        #selects only the rows of \"res1\" where the ticker name is present (or res[t1]=True)\n",
    "        t1_cols = res1[res1[ticker_title]].loc[:,['title','date']]\n",
    "        \n",
    "        #for all the columns sharing a date, merges the titles into a list. Add cols for date & ticker.\n",
    "        t1_conc_titles = t1_cols.groupby('date')['title'].apply(list).rename('list_titles')\n",
    "        t1_lengths = t1_conc_titles.agg(len).rename('length')\n",
    "        r1 = pd.concat([t1_conc_titles, t1_lengths], axis=1)\n",
    "        t1_res3 = r1.reset_index()\n",
    "        t1_res3['ticker'] = t1\n",
    "\n",
    "        ticker_frames.append(t1_res3)\n",
    "    res2 = pd.concat(ticker_frames,axis=0).reset_index(drop=True) ##this is ready for data analysis.\n",
    "    return res2\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------#\n",
    "### run all the functions down here\n",
    "\n",
    "#come up with more tickers\n",
    "tickers = ['facebook','netflix','apple','google','amazon', 'aramco', 'morgan stanley', 'chase'] \n",
    "\n",
    "#create columns for labeling each ticker within title.\n",
    "res1 = ticker_label_columns(tickers, rawitems_df)\n",
    "print(res1.head())\n",
    "\n",
    "#aggregate titles by ticker & date.\n",
    "print(\"\\n\\nres2:\")\n",
    "res2 = concat_titles_tk_date(res1, tickers)\n",
    "print(res2.head())\n",
    "\n",
    "\n",
    "### ^^ next, join the above with real stock movement data for the given dates.\n",
    "\n",
    "#then, save the result as a pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 2)\n",
      "(7, 11)\n",
      "                                                 title description  \\\n",
      "67    Facebook taking shady retailers 'very seriously'               \n",
      "155   Facebook taking shady retailers 'very seriously'               \n",
      "1646  Facebook taking shady retailers 'very seriously'               \n",
      "6266  Facebook taking shady retailers 'very seriously'               \n",
      "8127  Facebook taking shady retailers 'very seriously'               \n",
      "8990  Facebook taking shady retailers 'very seriously'               \n",
      "9622  Facebook taking shady retailers 'very seriously'               \n",
      "\n",
      "             date  facebook  netflix  apple  google  amazon  aramco  \\\n",
      "67    08 Apr 2016      True    False  False   False   False   False   \n",
      "155   08 Apr 2016      True    False  False   False   False   False   \n",
      "1646  08 Apr 2016      True    False  False   False   False   False   \n",
      "6266  08 Apr 2016      True    False  False   False   False   False   \n",
      "8127  08 Apr 2016      True    False  False   False   False   False   \n",
      "8990  08 Apr 2016      True    False  False   False   False   False   \n",
      "9622  08 Apr 2016      True    False  False   False   False   False   \n",
      "\n",
      "      morgan_stanley  chase  \n",
      "67             False  False  \n",
      "155            False  False  \n",
      "1646           False  False  \n",
      "6266           False  False  \n",
      "8127           False  False  \n",
      "8990           False  False  \n",
      "9622           False  False  \n",
      "------------------\n",
      "                                                   title         date\n",
      "67      Facebook taking shady retailers 'very seriously'  08 Apr 2016\n",
      "84                  'Chatbots' may be coming to Facebook  08 Apr 2016\n",
      "155     Facebook taking shady retailers 'very seriously'  08 Apr 2016\n",
      "172                 'Chatbots' may be coming to Facebook  08 Apr 2016\n",
      "252                 'Chatbots' may be coming to Facebook  08 Apr 2016\n",
      "386    Facebook live video set to beat rivals at own ...  08 Apr 2016\n",
      "644    Facebook live video set to beat rivals at own ...  08 Apr 2016\n",
      "1336   Facebook live video set to beat rivals at own ...  08 Apr 2016\n",
      "1479                'Chatbots' may be coming to Facebook  08 Apr 2016\n",
      "1646    Facebook taking shady retailers 'very seriously'  08 Apr 2016\n",
      "1663                'Chatbots' may be coming to Facebook  08 Apr 2016\n",
      "2244   Facebook live video set to beat rivals at own ...  08 Apr 2016\n",
      "5810   Facebook live video set to beat rivals at own ...  08 Apr 2016\n",
      "6266    Facebook taking shady retailers 'very seriously'  08 Apr 2016\n",
      "6288                'Chatbots' may be coming to Facebook  08 Apr 2016\n",
      "7088   Facebook live video set to beat rivals at own ...  08 Apr 2016\n",
      "8127    Facebook taking shady retailers 'very seriously'  08 Apr 2016\n",
      "8149                'Chatbots' may be coming to Facebook  08 Apr 2016\n",
      "8516                'Chatbots' may be coming to Facebook  08 Apr 2016\n",
      "8990    Facebook taking shady retailers 'very seriously'  08 Apr 2016\n",
      "9012                'Chatbots' may be coming to Facebook  08 Apr 2016\n",
      "9350   Facebook live video set to beat rivals at own ...  08 Apr 2016\n",
      "9471   Facebook live video set to beat rivals at own ...  08 Apr 2016\n",
      "9622    Facebook taking shady retailers 'very seriously'  08 Apr 2016\n",
      "9640                'Chatbots' may be coming to Facebook  08 Apr 2016\n",
      "10185               'Chatbots' may be coming to Facebook  08 Apr 2016\n",
      "10362  Facebook live video set to beat rivals at own ...  08 Apr 2016\n",
      "10489  Facebook live video set to beat rivals at own ...  08 Apr 2016\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "###############################\n",
    "## debug the list creation...\n",
    "###############################\n",
    "\n",
    "# April 8th: too many fb articles...\n",
    "\n",
    "#result... high #s of duplicates are coming from the data, not from aggregation.\n",
    "'''\n",
    "t1_cols = res1[res1['facebook']].loc[:,['title','date']]\n",
    "r1 = t1_cols[t1_cols.date == '08 Apr 2016']\n",
    "print(r1.shape)\n",
    "#print(r1)\n",
    "\n",
    "## q: is the problem present in res1?\n",
    "r2 = res1[res1.title == \"Facebook taking shady retailers 'very seriously'\"]\n",
    "print(r2.shape)\n",
    "print(r2)\n",
    "\n",
    "print(\"------------------\")\n",
    "print(r1)\n",
    "\n",
    "#for all the columns sharing a date, merges the titles into a list. Add cols for date & ticker.\n",
    "#t1_conc_titles = t1_cols.groupby('date')['title'].apply(list).rename('list_titles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             date    Open   Close    ticker\n",
      "1258  16 Dec 2019  195.27  197.92  facebook\n",
      "1259  13 Dec 2019  196.40  194.11  facebook\n",
      "1260  12 Dec 2019  202.35  196.75  facebook\n",
      "1261  11 Dec 2019  200.28  202.26  facebook\n",
      "1262  10 Dec 2019  201.66  200.87  facebook\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    In this cell, pull in the stock market data from csv files\n",
    "'''\n",
    "def extract_historical_stocks(stocks_data_dir):\n",
    "    stocks_files = os.listdir(stocks_data_dir)\n",
    "\n",
    "    #for each file, make the file into a pandas dataframe.\n",
    "    df = pd.DataFrame()\n",
    "    all_tkr_dfs = []\n",
    "    for sfile in stocks_files:\n",
    "        r1 = df.copy()\n",
    "\n",
    "        full_path = stocks_data_dir + '/' + sfile\n",
    "        stockdf1 = pd.read_csv(full_path)\n",
    "        sfile_tkr = sfile[:-4]\n",
    "        # print(stockdf1.head())\n",
    "\n",
    "        #convert dates to the right format\n",
    "        dates = stockdf1['Date'].apply(lambda x: dt.strptime(x,'%m/%d/%Y').strftime('%d %b %Y')).rename('date')\n",
    "        #require a space (' ') before the key name... Also column names: \"stockdf.columns\"\n",
    "        #here we convert string stock price to float.\n",
    "        close_prc = stockdf1[' Close/Last'].apply(lambda x: float(x.strip()[1:])).rename('Close')\n",
    "        open_prc = stockdf1[' Open'].apply(lambda x: float(x.strip()[1:])).rename('Open')\n",
    "\n",
    "        r1 = pd.concat([dates, open_prc, close_prc], axis=1)\n",
    "        r1['ticker'] = sfile_tkr\n",
    "        all_tkr_dfs.append(r1)\n",
    "        #print(r1)\n",
    "\n",
    "    #combine everything at the end.\n",
    "    historical_stock_prices = pd.concat(all_tkr_dfs, axis=0).reset_index(drop=True)\n",
    "    return historical_stock_prices\n",
    "\n",
    "\n",
    "stocks_data_dir = '../stock_data'\n",
    "historical_stock_prices = extract_historical_stocks(stocks_data_dir)\n",
    "print(historical_stock_prices[historical_stock_prices.ticker=='facebook'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          date                                        list_titles  length  \\\n",
      "0  06 Apr 2016  [Zuck tries Facebook Live ... doesn't go well,...     4.0   \n",
      "1  07 Apr 2016  [The firm helping Facebook get $$$ in lost rev...     3.0   \n",
      "2  08 Apr 2016  [Facebook taking shady retailers 'very serious...    28.0   \n",
      "3  19 Apr 2016  [How one trader plans to make millions on Face...    18.0   \n",
      "4  08 Apr 2016  [Actually, Netflix did warn about this price h...     9.0   \n",
      "\n",
      "     ticker    Open   Close  \n",
      "0  facebook  112.47  113.71  \n",
      "1  facebook  113.79  113.64  \n",
      "2  facebook  114.25  110.63  \n",
      "3  facebook  111.10  112.29  \n",
      "4   netflix  105.12  103.81  \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Utilize joins to say which days the stock price dropped and when it gained. Once this is combined with the\n",
    "    features, we will have the full \"trdata\" database created.\n",
    "    \n",
    "    TODO: Run all the above code for all files (once crawled) and save to a pickle.\n",
    "'''\n",
    "#df1: historical_stock_prices; df2: \"res2\" - converted the raw data to \"training\" format.\n",
    "r1 = historical_stock_prices\n",
    "trdata = res2.join(r1.set_index(['date','ticker']), on=['date', 'ticker'])\n",
    "print(trdata.head())\n",
    "\n",
    "### now we finally have the final form of \"trdata\"!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## store the result from above as a pickle.\n",
    "import pickle as pkl\n",
    "with open('./pandas_trdata/trdata.pickle','wb') as f:\n",
    "    pkl.dump(trdata, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below, explore the full Training Data just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fb: 53\n",
      "netflix: 366\n",
      "google: 100\n",
      "apple: 181\n",
      "amzn: 152\n",
      "\n",
      "(10660, 11)\n",
      "(26, 4)\n"
     ]
    }
   ],
   "source": [
    "## get some stats for the data we just collected...\n",
    "print('fb: ',end='')\n",
    "print(sum(res1['facebook']))\n",
    "\n",
    "print('netflix: ',end='')\n",
    "print(sum(res1['netflix']))\n",
    "\n",
    "print('google: ',end='')\n",
    "print(sum(res1['google']))\n",
    "\n",
    "print('apple: ',end='')\n",
    "print(sum(res1['apple']))\n",
    "\n",
    "print('amzn: ',end='')\n",
    "print(sum(res1['amazon']))\n",
    "\n",
    "## more data analysis...\n",
    "print(\"\")\n",
    "print(res1.shape)\n",
    "\n",
    "print(res2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   list_titles  length\n",
      "date                                                                  \n",
      "01 Jul 2016  [Ex-Facebooker reveals how the ‘cult’ of Silic...       3\n",
      "02 Jun 2016  [Gawker founder says Facebook should be held a...       1\n",
      "03 Jun 2016  [Facebook board seeks curb in Zuckerberg contr...       1\n",
      "05 Jul 2016  [Twitter adds ex-Facebook technology chief to ...       1\n",
      "06 May 2016  [Facebook's 'screen queen' leaving Oculus, Fac...       2\n",
      "07 Apr 2016  [The firm helping Facebook get $$$ in lost rev...       2\n",
      "07 Jul 2016           [DOJ sues to enforce Facebook tax probe]       1\n",
      "08 Apr 2016  [Facebook taking shady retailers 'very serious...       4\n",
      "11 Jun 2016  ['Moments' in time: Facebook tells users to do...       2\n",
      "13 Jul 2016  [Inside the secret lab where Facebook tries to...       1\n",
      "13 May 2016  [Zuckerberg says Facebook probing claims ‘tren...       1\n",
      "16 Jun 2016  [Short Apple, long Samsung and Facebook could ...       1\n",
      "18 Apr 2016  [China’s WeChat better than Facebook: Advertis...       1\n",
      "19 Apr 2016  [Facebook may soon let users collect $$$ on po...       1\n",
      "19 May 2016  [Kudlow: Facebook employees donate more to Cli...       1\n",
      "24 Aug 2016  [Liberal, moderate or conservative? See how Fa...       1\n",
      "25 Jul 2016  [Twitter bets on live events to spur growth, t...       2\n",
      "25 May 2016  [Move over Apple, Facebook is Wall Street's ne...       1\n",
      "27 Jun 2016  [Google, Facebook, Netflix could face Brexit g...       1\n",
      "27 May 2016  [Mark Zuckerberg is ‘dictator’ of Facebook ‘na...       2\n",
      "27 Sep 2016  [Facebook millionaire Luckey aligns himself wi...       1\n",
      "30 Jun 2016  [Apple, Google, Facebook, Amazon, Microsoft wo...       2\n"
     ]
    }
   ],
   "source": [
    "#res2\n",
    "\n",
    "t1_cols = res1[res1['facebook']].loc[:,['title','date']]\n",
    "r2 = t1_cols.groupby('date')['title'].apply(list).rename('list_titles')\n",
    "#print(r2)\n",
    "#print('#--------------------------------------#')\n",
    "r3 = r2.agg(len).rename('length')\n",
    "\n",
    "r4 = pd.concat([r2, r3], axis=1)\n",
    "\n",
    "print(r4)\n",
    "\n",
    "# [this got the length...] t1_cols.groupby('date')['title'].apply(list).agg(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                title  \\\n",
      "2   \\n      Apple's Revenue Falls Short, Shares Di...   \n",
      "23  \\n      Jobs Threatened Suit to Prevent Apple ...   \n",
      "\n",
      "                                         date  \n",
      "2   \\n      Wed, 23 Jan 2013 22:42 GMT\\n       \n",
      "23  \\n      Wed, 23 Jan 2013 13:48 GMT\\n       \n",
      "#--------------------------------------------#\n",
      "\n",
      "      Wed, 23 Jan 2013 13:48 GMT\n",
      "     \n",
      "                                                title  \\\n",
      "23  \\n      Jobs Threatened Suit to Prevent Apple ...   \n",
      "\n",
      "                                         date  \n",
      "23  \\n      Wed, 23 Jan 2013 13:48 GMT\\n       \n",
      "\n",
      "      Wed, 23 Jan 2013 22:42 GMT\n",
      "     \n",
      "                                               title  \\\n",
      "2  \\n      Apple's Revenue Falls Short, Shares Di...   \n",
      "\n",
      "                                        date  \n",
      "2  \\n      Wed, 23 Jan 2013 22:42 GMT\\n       \n",
      "#--------------------------------------------#\n",
      "                                        date  \\\n",
      "0  \\n      Wed, 23 Jan 2013 13:48 GMT\\n        \n",
      "1  \\n      Wed, 23 Jan 2013 22:42 GMT\\n        \n",
      "\n",
      "                                               title ticker  \n",
      "0  [\\n      Jobs Threatened Suit to Prevent Apple...  apple  \n",
      "1  [\\n      Apple's Revenue Falls Short, Shares D...  apple  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#res1.groupby(['apple','date']).filter('apple')\n",
    "\n",
    "#date: \\n Wed, 23 Jan 2013 22:42 GMT\\n\n",
    "res2 = res1[res1['apple']].loc[:,['title','date']]\n",
    "print(res2)\n",
    "\n",
    "print('#--------------------------------------------#')\n",
    "for name, group in res2.groupby('date'):\n",
    "    print(name)\n",
    "    print(group)\n",
    "\n",
    "    \n",
    "    \n",
    "#the below is pretty close. Figure out how to add the ticker back into it.\n",
    "print('#--------------------------------------------#')\n",
    "r3 = res2.groupby('date')['title'].apply(list)\n",
    "res3 = r3.reset_index()\n",
    "res3['ticker']='apple'\n",
    "    ## come up with one of these per group, and we will be done.\n",
    "print(res3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "### do some data cleaning\n",
    "a = 5\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
