{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['733_rss_20160519', '5292_rss_20141227', '873_rss_20160527', '1337_rss_20160629', '5846_rss_20141206', '1182_rss_20160620', '6249_rss_20160420', '1074_rss_20160615', '2473_rss_20160408', '3683_rss_20160630', '1582_rss_20160713', '3042_rss_20160521', '2735_rss_20160422', '4536_rss_20160608', '2760_rss_20160422', '2362_rss_20140107', '1850_rss_20160824', '3763_rss_20160705', '6482_rss_20160510', '6662_rss_20160519']\n",
      "                                               title description         date\n",
      "0           Traders: Here's how we're playing retail              19 May 2016\n",
      "1  The Fed brought this fear back to markets this...              19 May 2016\n",
      "2  What we know about missing EgyptAir Flight MS8...              19 May 2016\n",
      "3     CIO: Investors in these stocks, beware of this              19 May 2016\n",
      "4  Theranos CEO is looking for a new executive as...              19 May 2016\n",
      "(29929, 3)\n"
     ]
    }
   ],
   "source": [
    "'''goal of this notebook: use pandas to organize the data into training data.'''\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from datetime import datetime as dt\n",
    "\n",
    "#set directory and extract file contents in a loop\n",
    "#datadir = './test_rss' (for test files)\n",
    "datadir = '../crawl_data/rawdata_archived_rss_files'\n",
    "datafiles = os.listdir(datadir)\n",
    "print(datafiles[:20])\n",
    "max_tr_limit = 1000\n",
    "\n",
    "### open a file\n",
    "#f = open(datadir + '/' + '733_rss_20160519')\n",
    "#print(f.read())\n",
    "\n",
    "list_dfs = []\n",
    "for i, fname in enumerate(datafiles):\n",
    "    if i >= max_tr_limit:\n",
    "        break\n",
    "    \n",
    "    #open an rss file, convert its html elements to beautiful soup, and extract all links and metadata.\n",
    "    full_fpath = datadir + '/' + fname\n",
    "    with open(full_fpath,'rb') as f:\n",
    "        soup = BeautifulSoup(f.read())\n",
    "        #print(soup)\n",
    "\n",
    "        #for each element, need to store it in pandas form. {Can I do this any faster???}\n",
    "        items = soup.findAll(\"item\")\n",
    "        for item in items:\n",
    "            #note: was not able to get the link. That will require some extra work.\n",
    "            df_item = pd.DataFrame({\n",
    "                'title':[item.title.text.strip()],\n",
    "                'description':[item.description.text.strip()],\n",
    "                'date':[item.pubdate.text.strip()[5:16]]\n",
    "            })\n",
    "\n",
    "            #aggregate all \"item\"-dataframes.\n",
    "            list_dfs.append(df_item)\n",
    "\n",
    "rawitems_df = pd.concat(list_dfs, axis=0).reset_index(drop=True)\n",
    "print(rawitems_df.head())\n",
    "print(rawitems_df.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    \"sql\" steps:\n",
    "    1. for each ticker, pull out all articles related to that ticker.\n",
    "        i. ~ create new columns in our dataframe to say which tickers belong to which titles.\n",
    "        ii. also, need to figure out which points have enough data.\n",
    "    \n",
    "    2. reorganize the dataframe:\n",
    "        i. group by ticker and date. When you do this, every group will be a new data point.\n",
    "        ii. Combine this with the stock movement of the given stock on the given day.\n",
    "        iii. Now we should be ready for data analysis.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title description         date  \\\n",
      "0           Traders: Here's how we're playing retail              19 May 2016   \n",
      "1  The Fed brought this fear back to markets this...              19 May 2016   \n",
      "2  What we know about missing EgyptAir Flight MS8...              19 May 2016   \n",
      "3     CIO: Investors in these stocks, beware of this              19 May 2016   \n",
      "4  Theranos CEO is looking for a new executive as...              19 May 2016   \n",
      "\n",
      "   facebook  netflix  apple  google  amazon  aramco  morgan_stanley  chase  \n",
      "0     False    False  False   False   False   False           False  False  \n",
      "1     False    False  False   False   False   False           False  False  \n",
      "2     False    False  False   False   False   False           False  False  \n",
      "3     False    False  False   False   False   False           False  False  \n",
      "4     False    False  False   False   False   False           False  False  \n",
      "\n",
      "\n",
      "res2:\n",
      "          date                                        list_titles  length  \\\n",
      "0  01 Jul 2016  [Ex-Facebooker reveals how the ‘cult’ of Silic...      13   \n",
      "1  02 Jun 2016  [Gawker founder says Facebook should be held a...       1   \n",
      "2  03 Aug 2016  [Facebook squares off against Twitter, Snapcha...       8   \n",
      "3  03 Jun 2016  [Facebook board seeks curb in Zuckerberg contr...       2   \n",
      "4  04 Sep 2015  [Five details you shouldn't give Facebook, Fac...       2   \n",
      "\n",
      "     ticker  \n",
      "0  facebook  \n",
      "1  facebook  \n",
      "2  facebook  \n",
      "3  facebook  \n",
      "4  facebook  \n"
     ]
    }
   ],
   "source": [
    "### Q: what is the best way to search within pandas?? \n",
    "##  https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.contains.html\n",
    "'''\n",
    "    This cell: convert the html data from raw data to data that is ready for ML:\n",
    "    Namely, in our input data for ML, we want one row for each ticker and each day.\n",
    "'''\n",
    "\n",
    "'''\n",
    "    Search for tickers within titles, and create a new column for each ticker.\n",
    "'''\n",
    "def ticker_label_columns(tickers, rawitems_df):\n",
    "    l1 = []\n",
    "    contains_ticker_cols = [rawitems_df]\n",
    "    for t1 in tickers:\n",
    "        contains_ti = l1.copy()\n",
    "        ticker_title = t1.replace(' ','_') #come up with another function, or a dictionary?\n",
    "        contains_ti = rawitems_df['title'].str.contains(t1, case=False, regex=False).rename(ticker_title)\n",
    "        contains_ticker_cols.append(contains_ti)\n",
    "    res1 = pd.concat(contains_ticker_cols, axis=1)\n",
    "    return res1\n",
    "\n",
    "\n",
    "'''\n",
    "    Group by ticker and date. Combine all the titles into a single list of strings type.\n",
    "    ##! need to test this out on a bigger dataset\n",
    "'''\n",
    "def concat_titles_tk_date(res1, tickers):\n",
    "    ticker_frames = []\n",
    "    for t1 in tickers:\n",
    "        ticker_title = t1.replace(' ','_') ## need something better!\n",
    "        #selects only the rows of \"res1\" where the ticker name is present (or res[t1]=True)\n",
    "        t1_cols = res1[res1[ticker_title]].loc[:,['title','date']]\n",
    "        \n",
    "        #for all the columns sharing a date, merges the titles into a list. Add cols for date & ticker.\n",
    "        t1_conc_titles = t1_cols.groupby('date')['title'].apply(list).rename('list_titles')\n",
    "        t1_lengths = t1_conc_titles.agg(len).rename('length')\n",
    "        r1 = pd.concat([t1_conc_titles, t1_lengths], axis=1)\n",
    "        t1_res3 = r1.reset_index()\n",
    "        t1_res3['ticker'] = t1\n",
    "\n",
    "        ticker_frames.append(t1_res3)\n",
    "    res2 = pd.concat(ticker_frames,axis=0).reset_index(drop=True) ##this is ready for data analysis.\n",
    "    return res2\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------#\n",
    "### run all the functions down here\n",
    "\n",
    "#come up with more tickers\n",
    "tickers = ['facebook','netflix','apple','google','amazon', 'aramco', 'morgan stanley', 'chase'] \n",
    "\n",
    "#create columns for labeling each ticker within title.\n",
    "res1 = ticker_label_columns(tickers, rawitems_df)\n",
    "print(res1.head())\n",
    "\n",
    "#aggregate titles by ticker & date.\n",
    "print(\"\\n\\nres2:\")\n",
    "res2 = concat_titles_tk_date(res1, tickers)\n",
    "print(res2.head())\n",
    "\n",
    "\n",
    "### ^^ next, join the above with real stock movement data for the given dates.\n",
    "\n",
    "#then, save the result as a pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 2)\n",
      "(2, 11)\n",
      "                                                 title description  \\\n",
      "1506  Facebook taking shady retailers 'very seriously'               \n",
      "2853  Facebook taking shady retailers 'very seriously'               \n",
      "\n",
      "             date  facebook  netflix  apple  google  amazon  aramco  \\\n",
      "1506  08 Apr 2016      True    False  False   False   False   False   \n",
      "2853  08 Apr 2016      True    False  False   False   False   False   \n",
      "\n",
      "      morgan_stanley  chase  \n",
      "1506           False  False  \n",
      "2853           False  False  \n",
      "------------------\n",
      "                                                   title         date\n",
      "1506    Facebook taking shady retailers 'very seriously'  08 Apr 2016\n",
      "1523                'Chatbots' may be coming to Facebook  08 Apr 2016\n",
      "2853    Facebook taking shady retailers 'very seriously'  08 Apr 2016\n",
      "2870                'Chatbots' may be coming to Facebook  08 Apr 2016\n",
      "6908                'Chatbots' may be coming to Facebook  08 Apr 2016\n",
      "8632   Facebook live video set to beat rivals at own ...  08 Apr 2016\n",
      "14855  Facebook live video set to beat rivals at own ...  08 Apr 2016\n",
      "27198  Facebook live video set to beat rivals at own ...  08 Apr 2016\n",
      "29080               'Chatbots' may be coming to Facebook  08 Apr 2016\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "###############################\n",
    "## debug the list creation...\n",
    "###############################\n",
    "\n",
    "# April 8th: too many fb articles...\n",
    "\n",
    "#result... high #s of duplicates are coming from the data, not from aggregation.\n",
    "'''\n",
    "t1_cols = res1[res1['facebook']].loc[:,['title','date']]\n",
    "r1 = t1_cols[t1_cols.date == '08 Apr 2016']\n",
    "print(r1.shape)\n",
    "#print(r1)\n",
    "\n",
    "## q: is the problem present in res1?\n",
    "r2 = res1[res1.title == \"Facebook taking shady retailers 'very seriously'\"]\n",
    "print(r2.shape)\n",
    "print(r2)\n",
    "\n",
    "print(\"------------------\")\n",
    "print(r1)\n",
    "\n",
    "#for all the columns sharing a date, merges the titles into a list. Add cols for date & ticker.\n",
    "#t1_conc_titles = t1_cols.groupby('date')['title'].apply(list).rename('list_titles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             date    Open   Close    ticker\n",
      "1258  16 Dec 2019  195.27  197.92  facebook\n",
      "1259  13 Dec 2019  196.40  194.11  facebook\n",
      "1260  12 Dec 2019  202.35  196.75  facebook\n",
      "1261  11 Dec 2019  200.28  202.26  facebook\n",
      "1262  10 Dec 2019  201.66  200.87  facebook\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    In this cell, pull in the stock market data from csv files\n",
    "'''\n",
    "def extract_historical_stocks(stocks_data_dir):\n",
    "    stocks_files = os.listdir(stocks_data_dir)\n",
    "\n",
    "    #for each file, make the file into a pandas dataframe.\n",
    "    df = pd.DataFrame()\n",
    "    all_tkr_dfs = []\n",
    "    for sfile in stocks_files:\n",
    "        r1 = df.copy()\n",
    "\n",
    "        full_path = stocks_data_dir + '/' + sfile\n",
    "        stockdf1 = pd.read_csv(full_path)\n",
    "        sfile_tkr = sfile[:-4]\n",
    "        # print(stockdf1.head())\n",
    "\n",
    "        #convert dates to the right format\n",
    "        dates = stockdf1['Date'].apply(lambda x: dt.strptime(x,'%m/%d/%Y').strftime('%d %b %Y')).rename('date')\n",
    "        #require a space (' ') before the key name... Also column names: \"stockdf.columns\"\n",
    "        #here we convert string stock price to float.\n",
    "        close_prc = stockdf1[' Close/Last'].apply(lambda x: float(x.strip()[1:])).rename('Close')\n",
    "        open_prc = stockdf1[' Open'].apply(lambda x: float(x.strip()[1:])).rename('Open')\n",
    "\n",
    "        r1 = pd.concat([dates, open_prc, close_prc], axis=1)\n",
    "        r1['ticker'] = sfile_tkr\n",
    "        all_tkr_dfs.append(r1)\n",
    "        #print(r1)\n",
    "\n",
    "    #combine everything at the end.\n",
    "    historical_stock_prices = pd.concat(all_tkr_dfs, axis=0).reset_index(drop=True)\n",
    "    return historical_stock_prices\n",
    "\n",
    "\n",
    "stocks_data_dir = '../crawl_data/stock_data'\n",
    "historical_stock_prices = extract_historical_stocks(stocks_data_dir)\n",
    "print(historical_stock_prices[historical_stock_prices.ticker=='facebook'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          date                                        list_titles  length  \\\n",
      "0  01 Jul 2016  [Ex-Facebooker reveals how the ‘cult’ of Silic...      13   \n",
      "1  02 Jun 2016  [Gawker founder says Facebook should be held a...       1   \n",
      "2  03 Aug 2016  [Facebook squares off against Twitter, Snapcha...       8   \n",
      "3  03 Jun 2016  [Facebook board seeks curb in Zuckerberg contr...       2   \n",
      "4  04 Sep 2015  [Five details you shouldn't give Facebook, Fac...       2   \n",
      "\n",
      "     ticker     Open   Close  \n",
      "0  facebook  114.200  114.19  \n",
      "1  facebook  118.690  118.93  \n",
      "2  facebook  123.090  122.51  \n",
      "3  facebook  118.975  118.47  \n",
      "4  facebook   87.200   88.26  \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Utilize joins to say which days the stock price dropped and when it gained. Once this is combined with the\n",
    "    features, we will have the full \"trdata\" database created.\n",
    "    \n",
    "    TODO: Run all the above code for all files (once crawled) and save to a pickle.\n",
    "'''\n",
    "#df1: historical_stock_prices; df2: \"res2\" - converted the raw data to \"training\" format.\n",
    "r1 = historical_stock_prices\n",
    "trdata = res2.join(r1.set_index(['date','ticker']), on=['date', 'ticker'])\n",
    "print(trdata.head())\n",
    "\n",
    "### now we finally have the final form of \"trdata\"!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## store the result from above as a pickle.\n",
    "import pickle as pkl\n",
    "with open('./pandas_trdata/trdata.pickle','wb') as f:\n",
    "    pkl.dump(trdata, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below, explore the full Training Data just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fb: 206\n",
      "netflix: 100\n",
      "google: 378\n",
      "apple: 515\n",
      "amzn: 207\n",
      "\n",
      "(29929, 11)\n",
      "(433, 4)\n"
     ]
    }
   ],
   "source": [
    "## get some stats for the data we just collected...\n",
    "print('fb: ',end='')\n",
    "print(sum(res1['facebook']))\n",
    "\n",
    "print('netflix: ',end='')\n",
    "print(sum(res1['netflix']))\n",
    "\n",
    "print('google: ',end='')\n",
    "print(sum(res1['google']))\n",
    "\n",
    "print('apple: ',end='')\n",
    "print(sum(res1['apple']))\n",
    "\n",
    "print('amzn: ',end='')\n",
    "print(sum(res1['amazon']))\n",
    "\n",
    "## more data analysis...\n",
    "print(\"\")\n",
    "print(res1.shape)\n",
    "\n",
    "print(res2.shape)\n",
    "\n",
    "### problem: this is way too little training data to train our lstm. How can we get more?\n",
    "### we will have to collect at least 2-3k date/ticker combos total. \n",
    "\n",
    "'''\n",
    "    problem: 2016 has 433 date-ticker pairs. In order to train a satisfactory LSTM, we need\n",
    "    around 10k data points (best case scenario). A minimum threshold would be around 2-3k examples...\n",
    "    \n",
    "    solution? \n",
    "    1. Think about looking into some low-data approaches?\n",
    "    2. data augmentation?\n",
    "        ~ Can we change it so it's no longer a list? We would need to eliminate dupes...\n",
    "    3. try it out anyway\n",
    "    4. need some other way to collect news headlines.\n",
    "    \n",
    "    \n",
    "    Here are some other sources for news data:\n",
    "    0. A kaggle competition was done on this exact topic: https://www.kaggle.com/c/two-sigma-financial-news/rules\n",
    "    1. reuters: https://www.reutersagency.com/en/contact/\n",
    "    2. A dataset of full articles!: https://www.kaggle.com/snapcrack/all-the-news\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   list_titles  length\n",
      "date                                                                  \n",
      "01 Jul 2016  [Ex-Facebooker reveals how the ‘cult’ of Silic...       3\n",
      "02 Jun 2016  [Gawker founder says Facebook should be held a...       1\n",
      "03 Jun 2016  [Facebook board seeks curb in Zuckerberg contr...       1\n",
      "05 Jul 2016  [Twitter adds ex-Facebook technology chief to ...       1\n",
      "06 May 2016  [Facebook's 'screen queen' leaving Oculus, Fac...       2\n",
      "07 Apr 2016  [The firm helping Facebook get $$$ in lost rev...       2\n",
      "07 Jul 2016           [DOJ sues to enforce Facebook tax probe]       1\n",
      "08 Apr 2016  [Facebook taking shady retailers 'very serious...       4\n",
      "11 Jun 2016  ['Moments' in time: Facebook tells users to do...       2\n",
      "13 Jul 2016  [Inside the secret lab where Facebook tries to...       1\n",
      "13 May 2016  [Zuckerberg says Facebook probing claims ‘tren...       1\n",
      "16 Jun 2016  [Short Apple, long Samsung and Facebook could ...       1\n",
      "18 Apr 2016  [China’s WeChat better than Facebook: Advertis...       1\n",
      "19 Apr 2016  [Facebook may soon let users collect $$$ on po...       1\n",
      "19 May 2016  [Kudlow: Facebook employees donate more to Cli...       1\n",
      "24 Aug 2016  [Liberal, moderate or conservative? See how Fa...       1\n",
      "25 Jul 2016  [Twitter bets on live events to spur growth, t...       2\n",
      "25 May 2016  [Move over Apple, Facebook is Wall Street's ne...       1\n",
      "27 Jun 2016  [Google, Facebook, Netflix could face Brexit g...       1\n",
      "27 May 2016  [Mark Zuckerberg is ‘dictator’ of Facebook ‘na...       2\n",
      "27 Sep 2016  [Facebook millionaire Luckey aligns himself wi...       1\n",
      "30 Jun 2016  [Apple, Google, Facebook, Amazon, Microsoft wo...       2\n"
     ]
    }
   ],
   "source": [
    "#res2\n",
    "\n",
    "t1_cols = res1[res1['facebook']].loc[:,['title','date']]\n",
    "r2 = t1_cols.groupby('date')['title'].apply(list).rename('list_titles')\n",
    "#print(r2)\n",
    "#print('#--------------------------------------#')\n",
    "r3 = r2.agg(len).rename('length')\n",
    "\n",
    "r4 = pd.concat([r2, r3], axis=1)\n",
    "\n",
    "print(r4)\n",
    "\n",
    "# [this got the length...] t1_cols.groupby('date')['title'].apply(list).agg(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                title  \\\n",
      "2   \\n      Apple's Revenue Falls Short, Shares Di...   \n",
      "23  \\n      Jobs Threatened Suit to Prevent Apple ...   \n",
      "\n",
      "                                         date  \n",
      "2   \\n      Wed, 23 Jan 2013 22:42 GMT\\n       \n",
      "23  \\n      Wed, 23 Jan 2013 13:48 GMT\\n       \n",
      "#--------------------------------------------#\n",
      "\n",
      "      Wed, 23 Jan 2013 13:48 GMT\n",
      "     \n",
      "                                                title  \\\n",
      "23  \\n      Jobs Threatened Suit to Prevent Apple ...   \n",
      "\n",
      "                                         date  \n",
      "23  \\n      Wed, 23 Jan 2013 13:48 GMT\\n       \n",
      "\n",
      "      Wed, 23 Jan 2013 22:42 GMT\n",
      "     \n",
      "                                               title  \\\n",
      "2  \\n      Apple's Revenue Falls Short, Shares Di...   \n",
      "\n",
      "                                        date  \n",
      "2  \\n      Wed, 23 Jan 2013 22:42 GMT\\n       \n",
      "#--------------------------------------------#\n",
      "                                        date  \\\n",
      "0  \\n      Wed, 23 Jan 2013 13:48 GMT\\n        \n",
      "1  \\n      Wed, 23 Jan 2013 22:42 GMT\\n        \n",
      "\n",
      "                                               title ticker  \n",
      "0  [\\n      Jobs Threatened Suit to Prevent Apple...  apple  \n",
      "1  [\\n      Apple's Revenue Falls Short, Shares D...  apple  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#res1.groupby(['apple','date']).filter('apple')\n",
    "\n",
    "#date: \\n Wed, 23 Jan 2013 22:42 GMT\\n\n",
    "res2 = res1[res1['apple']].loc[:,['title','date']]\n",
    "print(res2)\n",
    "\n",
    "print('#--------------------------------------------#')\n",
    "for name, group in res2.groupby('date'):\n",
    "    print(name)\n",
    "    print(group)\n",
    "\n",
    "    \n",
    "    \n",
    "#the below is pretty close. Figure out how to add the ticker back into it.\n",
    "print('#--------------------------------------------#')\n",
    "r3 = res2.groupby('date')['title'].apply(list)\n",
    "res3 = r3.reset_index()\n",
    "res3['ticker']='apple'\n",
    "    ## come up with one of these per group, and we will be done.\n",
    "print(res3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "### do some data cleaning\n",
    "a = 5\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1446\n"
     ]
    }
   ],
   "source": [
    "## check out the earlier pickle that I created...\n",
    "### 1446 headlines... I don't know what happened to the 2018 and 2019 data...\n",
    "import pickle as pkl\n",
    "unpickle = False\n",
    "if unpickle:\n",
    "    fdir = '../crawl_data/rss_feed_articles.pickle'\n",
    "    with open(fdir, 'rb') as f:\n",
    "        data = pkl.load(f)\n",
    "    \n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
